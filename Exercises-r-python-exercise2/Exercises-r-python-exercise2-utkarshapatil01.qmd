---
title: "Exercises-r-python-exercise2"
author: Utkarsha Patil
format: html
editor: visual
---

# **Data Preprocessing in R**

## **Installing required packages**

```{r warning=FALSE, error=FALSE}
# First run this
if (!require("pacman"))
install.packages("pacman")
```

```{r message=FALSE, error=FALSE}
library(pacman)

p_load(DBI, # DBI databases
       dlookr,
       here, # Reproducible/ standard directories
       janitor,
       RMySQL, # Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       tidyverse, # Data wrangling, manipulation, visualization
       qqplotr) 

```

## **Loading data**

### **CSV files (`.csv`)**

```{r}
data <- read_csv("x.csv")

data |> glimpse() # concise summary of the data frame
```

### **Tab separated values (`x.tsv`)**

```{r}
data <- read_delim("x.tsv")

data |> glimpse() # concise summary of the data frame
```

## **Importing data from MySQL database**

```{r}
drv <- dbDriver("MySQL") #obtain the driver for MySQL, drivers available for other DBMS
```

### **Using `dplyr` instead**

```{r message=FALSE, error=FALSE}
if (!require("dbplyr"))
install.packages("dbplyr") 
```

### **Obtain a connection**

Creating connection to database 'mydatabase' on localhost with the help of credentials.

```{r warning=FALSE, message=FALSE, error=FALSE}
con <- src_mysql("mydatabase", user = "root", password = "utkpat", host = "localhost")
```

```{r warning=FALSE}
employees <- tbl(con, "employees")
employees
```

# **Data Cleaning**

Data cleaning is a critical step in the data preprocessing pipeline, which involves identifying and rectifying errors, inconsistencies, and missing values in your dataset to ensure that it is suitable for analysis or modeling.

```{r}
wide <- read_delim(here("wide.txt"), delim = " ", skip = 1, col_names = c("Name", "Math", "English", "Degree_Year"))
```

The "Math" and "English" columns will be combined into a single "Grade" column, and a new "Subject" column will indicate whether the grade corresponds to math or English.

```{r}
long <- wide |>
  pivot_longer(cols = c(Math, English),
               names_to = "Subject", 
               values_to = "Grade")
long
```

## **Long to wide, use `spread()`**

transform data from long format back to wide format.

```{r}
wide <- long %>%
  pivot_wider(names_from = Subject, values_from = Grade)
wide
```

## **Split a column into multiple columns**

```{r}
clean <- long %>%
  separate(Degree_Year, c("Degree", "Year"), sep = "_")

clean
```

## **Handling date/time and time zones**

```{r}
if (!require("lubridate"))
install.packages("lubridate")
library(lubridate)
```

Convert dates of variance formats into one format:

```{r}
mixed.dates <- c(20140123, "2019-12-12", "2009/5/1",
 "measured on 2002-12-06", "2018-7/16")
clean.dates <- ymd(mixed.dates) #convert to year-month-day format
clean.dates
```

Extract day, week, month, year info from dates:

```{r}
data.frame(Dates = clean.dates, WeekDay = wday(clean.dates), nWeekDay = wday(clean.dates, label = TRUE), Year = year(clean.dates), Month = month(clean.dates, label = TRUE))
```

Time zone:

```{r}
date.time <- ymd_hms("20190203 03:00:03", tz="Asia/Shanghai")
```

Convert to Phoenix, AZ time:

```{r}
with_tz(date.time, tz="America/Phoenix")
```

Change the timezone for a time:

```{r}
force_tz(date.time, "Turkey")
```

Check available time zones:

```{r}
OlsonNames()
```

## **String Processing**

```{r}
library(dplyr)
library(stringr)
library(readr)
```

Fetch data from a URL, form the URL using string functions:

```{r}
uci.repo <-"http://archive.ics.uci.edu/ml/machine-learning-databases/"

dataset <- "audiology/audiology.standardized"
```

`str_c`: string concatenation:

```{r}
dataF <- str_c(uci.repo, dataset, ".data")
namesF <- str_c(uci.repo, dataset, ".names")
dataF
```

Read the data file:

```{r}
data <- read_csv(url(dataF), col_names = FALSE, na="?")
```

```{r}
dim(data) # retrieve the dimensions
```

Read the name file line by line, put the lines in a vector:

```{r}
lines <- read_lines(url(namesF))

lines |> head()
```

```{r}
names <- lines[67:135]
names
```

```{r}
names <- str_split_fixed(names, ":", 2) #split on regular expression pattern ":", this function returns a matrix
names
```

Take the first column, which contains names:

```{r}
names <- names[,1]
names
```

Now clean up the names: trim spaces, remove `()`:

```{r}
names <-str_trim(names) |> str_replace_all("\\(|\\)", "") # we use a pipe, and another reg exp "\\(|\\)", \\ is the escape.
names
```

```{r}
colnames(data)[1:69] <- names
data
```

Rename the last two columns:

```{r}
colnames(data)[70:71] <- c("id", "class")
data
```

## **Dealing with unknown values**

```{r}
library(dplyr)

missing.value.rows <- data |>
  filter(!complete.cases(data))
missing.value.rows # Remove observations or columns with many NAs:
```

```{r}
data <- data %>%
  mutate(na_count = rowSums(is.na(data))) # sum of missing (NA) values for each row 
data
```

following data frame that shows the column names and their corresponding missing value counts, sorted in ascending order of missing values. This can be useful for identifying which columns have the most or least missing data in your dataset.

```{r}
data |>
  summarize(across(everything(), ~sum(is.na(.)), .names = "na_{.col}")) %>%
  pivot_longer(everything(), names_to = "column_name", values_to = "na_count") %>%
  arrange(na_count)
```

```{r}
data.bser.removed <- data %>%
  select(-8) %>%
  summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}"))
data.bser.removed
```

```{r}
data <- data %>%
  select(-matches("bser"))
```

### **Mistaken characters**

The class of the vector **`mistaken`** is "character" because it contains at least one character element (the "?" character).

```{r}
mistaken <- c(2, 3, 4, "?")
class(mistaken)
```

```{r}
fixed <- parse_integer(mistaken, na = '?') #  parse character vectors and convert them into integer values
fixed
```

```{r}
class(fixed)
```

### **Filling unknowns with most frequent values**

```{r}
if (!require("DMwR2"))
install.packages("DMwR2")
library(DMwR2)
data(algae, package = "DMwR2")
algae[48,] # retrieves and displays the 48th row of the "algae" dataset
```

The "mxPH" values will be plotted against expected values if they were normally distributed. Deviations from the reference line can indicate departures from normality.

```{r message=FALSE}
# plot a QQ plot of mxPH
if (!require("car"))
install.packages("car")
library(car)
ggplot(algae, aes(sample = mxPH)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of mxPH") 
```

The "mxPH" value in the 48th row replaced by the mean of the "mxPH" column (excluding missing values).

```{r}
if (!require("ggplot2"))
install.packages("ggplot2")
library(ggplot2)
library(dplyr)
algae <- algae |>
  mutate(mxPH = ifelse(row_number() == 48, mean(mxPH, na.rm = TRUE), mxPH))
algae
```

What about attribute `Chla`?

```{r}
ggplot(algae, aes(sample = Chla)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of Chla") 
```

```{r}
median(algae$Chla, na.rm = TRUE) #  calculates the median of the values in the "Chla" column while ignoring missing values.
```

```{r}
mean(algae$Chla, na.rm = TRUE)  #  calculates the mean of the values in the "Chla" column while ignoring missing values.
```

```{r}
algae <- algae |>
  mutate(Chla = if_else(is.na(Chla), median(Chla, na.rm = TRUE), Chla))
```

### **Filling unknowns using linear regression**

Linear regression can be used to estimate missing values based on the relationship between the variable with missing values and other relevant variables in your dataset.

```{r}
algae_numeric <- algae[, 4:18] %>%
  drop_na()  # Removes rows with NA values

cor_matrix <- algae_numeric |> correlate() |> plot()
```

```{r}
cor_matrix
```

```{r}
algae <- algae %>%
  filter(rowSums(is.na(.)) / ncol(.) < 0.2)#this is a method provided that selects the observations with 20% or move values as NAs. 

m = lm(PO4 ~ oPO4, data = algae)
lm(formula = PO4 ~ oPO4, data = algae)
```

```{r}
m |> 
  summary()
```

```{r}
m |> 
  summary() |> 
  tidy()
```

```{r}
algae$PO4
```

PO4 for observation 28 can then be filled with predicated value using the model

```{r}
algae <- algae %>%
  mutate(PO4 = ifelse(row_number() == 28, 42.897 + 1.293 * oPO4, PO4))
```

```{r}
res = resid(m)

oPO4_reduced <- algae %>%
  filter(row_number() != 28) %>%
  pull(oPO4)
```

```{r}
ggplot(data = data.frame(oPO4 = m$model$oPO4, res = res), aes(x = oPO4, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    x = "oPO4",
    y = "residuals",
    title = "Residual Plot"
  )
```

```{r}
fillPO4 <- function(x) {
  if_else(is.na(x), 42.897 + 1.293 * x, x)
}
#if x is not NA, return 42.897+1.293*x 
```

```{r}
algae[is.na(algae$PO4), "PO4"] <- sapply(algae[is.na(algae$PO4), "oPO4"], fillPO4)
```

### **Filling unknowns by exploring similarities among cases**

common technique in data imputation, especially when you have missing data and you want to impute missing values based on the similarity between cases (observations) in your dataset. This technique is often referred to as "nearest neighbor imputation" or "k-nearest neighbors imputation."

```{r}
data(algae, package="DMwR2")
algae <- algae[-manyNAs(algae), ] 
```

```{r}
algae <- knnImputation(algae, k = 10) #use the weighted average of k most similar samples


data(algae, package="DMwR2") #get data again so there are unknown values
algae <- algae[-manyNAs(algae), ] 
algae <- knnImputation(algae, k = 10, meth="median") #use the median of k most similar samples
```

```{r}
getAnywhere(knnImputation())
```

```{r}
if (!require("palmerpenguins"))
install.packages("palmerpenguins")
library(dplyr)
library(palmerpenguins)
```

```{r}
data(penguins)
```

```{r}
# select only numeric columns
penguins_numeric <- select(penguins, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

# normalize numeric columns
penguins_norm <- scale(penguins_numeric)

# convert back to data frame and add species column
peng.norm <- cbind(as.data.frame(penguins_norm), species = penguins$species)

# because scale() takes numeric matrix as input, we first remove Species column, then use cbind() to add the column back after normalization.
```

```{r}
summary(penguins)
```

```{r}
summary(peng.norm)
```

```{r}
max <- apply(select(penguins, -species), 2, max, na.rm=TRUE)
min <- apply(select(penguins, -species), 2, min, na.rm=TRUE)
```

```{r}
max
```

```{r}
min
```

```{r}
# min-max normalization
penguin_scaled <- as.data.frame(lapply(penguins_numeric, function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))

penguin_scaled <- cbind(penguins_norm, species = penguins$species)

summary(penguin_scaled)
```

## **Discretizing variables (binning)**

Discretizing variables, also known as binning, is a technique used to convert continuous or numeric variables into categorical variables by dividing the data into bins or intervals.

```{r}
data(Boston, package="MASS")
summary(Boston$age)
```

```{r}
Boston$newAge <- dlookr::binning(Boston$age, 5, type = "equal") #create 5 bins and add new column newAge to Boston
summary(Boston$newAge)
```

```{r}
Boston$newAge <- dlookr::binning(Boston$age, nbins = 5, labels = c("very-young", "young", "mid", "older", "very-old"), type = "equal") #add labels

summary(Boston$newAge)
```

### **Equal-depth**

Equal-depth binning, also known as quantile-based binning, is a technique for dividing a continuous variable into bins or intervals such that each bin contains approximately the same number of observations.

```{r}
# Create equal-depth bins for the 'age' variable
Boston$newAge <- cut(Boston$age, breaks = 5)

# Display the result
table(Boston$newAge)

```

### **Assign labels**

```{r error=FALSE}
Boston$newAge <- factor(cut(Boston$age, breaks = 5), labels = c("very-young", "young", "mid", "older", "very-old"))

table(Boston$newAge)
```

Plot an equal-width histogram of width 10:

```{r}
hist(Boston$age, breaks = seq(0, 101,by = 10)) #seq() gives the function for breaks. The age ranges from 0 – 101.
```

```{r}
library(ggplot2)

Boston |>
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 10)
```

## **Decimal scaling**

Decimal scaling is a data normalization technique used to scale numerical data features to a common range, typically between -1 and 1 or -10 and 10, by moving the decimal point of each data point.

```{r}
data <- c(10, 20, 30, 50, 100)
```

```{r}
(nDigits = nchar(max(abs(data)))) #nchar counts the number of characters
```

```{r}
(decimalScale = data / (10^nDigits))
```

### **Smoothing by bin mean**

Smoothing by bin mean is a data preprocessing technique used to reduce the noise in a dataset by replacing each data point with the mean value of data points within a specific bin or interval. This technique is particularly useful when you have noisy data and want to create a smoother representation of the underlying trends in the data.

```{r}
age = c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30)

# Separate data into bins of depth 3
(bins = matrix(age, nrow = length(age) / 5, byrow = TRUE))
```

Find the average of each bin:

```{r}
(bin_means = apply(bins, 1, FUN = mean))
```

Replace values with their bin mean:

```{r}
for (i in 1:nrow(bins)) {
   bins[i,] = bin_means[i]
 }
bins
```

```{r}
(age_bin_mean_smoothed = round(as.vector(t(bins)), 2))
```

# **Variable correlations and dimensionality reduction**

## **Chi-squared test**

A chi-squared test is a statistical test used to determine whether there is a significant association between two categorical variables

```{r}
racetable = rbind(c(151,9), c(63,103))
test1 = chisq.test(racetable, correct=F)
test1
```

## **Loglinear model**

A loglinear model, also known as a log-linear model or logit model, is a statistical model used for analyzing relationships between categorical variables.

```{r}
seniors <- array(data = c(911, 44, 538, 456, 3, 2, 43, 279, 911, 44, 538, 456, 3, 2, 43, 279), 
                  dim = c(2, 2, 2, 2),
                  dimnames = list("cigarette" = c("yes", "no"),
                                  "marijuana" = c("yes", "no"),
                                  "alcohol" = c("yes", "no"), 
                                  "age" =c("younger", "older")))
```

```{r}
seniors
```

```{r}
seniors.tb <- as.table(seniors)
seniors.tb
```

```{r}
seniors.df <- as.data.frame(seniors.tb)
seniors.df
```

```{r}
mod.S4 <- glm(Freq ~ (cigarette * marijuana * alcohol * age), data = seniors.df, family=poisson)
summary(mod.S4)
```

```{r}
mod.S3 <- glm(Freq ~ (cigarette * marijuana * alcohol), data = seniors.df, family = poisson)
summary(mod.S3)
```

```{r}
mod.3 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, data = seniors.df, family = poisson)
summary(mod.3)
```

```{r}
cbind(mod.3$data, fitted(mod.3))
```

## **Correlations**

Correlation analysis is a statistical technique used to measure the strength and direction of the linear relationship between two or more continuous variables

```{r}
library(tidyr) # data manipulation
penguins_numeric |> 
  drop_na() |>
  correlate()
```

## **Principal components analysis (PCA)**

Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in statistics and data science to transform high-dimensional data into a lower-dimensional form while preserving as much of the original data's variance as possible.

```{r}
pca.data <- penguins |>
  drop_na() |>
  select(-species, -island, -sex) 

pca <- princomp(pca.data)
loadings(pca)
```

```{r}
head(pca$scores) # pca result is a list, and the component scores are elements in the list
```

```{r}
penguins_na <- penguins |> 
  drop_na()

peng.reduced <- data.frame(pca$scores[,1:3], Species = penguins_na$species)

head(peng.reduced)
```

```{r}
if (!require("wavelets"))
install.packages("wavelets")
library(wavelets)
```

```{r}
x <- c(2, 2, 0, 2, 3, 5, 4, 4)
wt <- dwt(x,filter="haar", n.levels = 3) #with 8-element vector, 3 level is the max.
wt
```

```{r}
idwt(wt)
```

Obtain transform results as shown in class, use a different filter:

```{r}
xt = dwt(x, filter = wt.filter(c(0.5, -0.5)), n.levels = 3)
xt
```

Reconstruct the original:

```{r}
idwt(xt)
```

# **Sampling**

```{r}
set.seed(1)
age <- c(25, 25, 25, 30, 33, 33, 35, 40, 45, 46, 52, 70)
```

## **Simple random sampling, without replacement:**

```{r}
sample(age, 5)
```

## **Simple random sampling, with replacement:**

```{r}
sample(age, 5, replace = TRUE)
```

## **Stratified sampling**

```{r}
library(dplyr)
set.seed(1) #make results the same each run
summary(algae)
```

```{r}
sample <-algae |> group_by(season) |> sample_frac(0.25)
summary(sample)
```

## **Cluster sampling**

```{r}
if (!require("sampling"))
install.packages("sampling")
library(sampling)
age <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
s <- kmeans(age, 3) #cluster on age to form 3 clusters
s$cluster
```

```{r warning=FALSE}
ageframe <- data.frame(age)
ageframe$condition <- s$cluster # add cluster label as condition
cluster(ageframe, clustername = "condition", size = 2) # select 2 clusters out of the three
```

# **Handling Text Datasets**

```{r}
pacman::p_load(tm,
               SnowballC)#tm uses SnowballC for stemming
# read corpus
# Emails.csv,  holding some of Hillary's emails
data <- read.csv(("Emails.csv"), stringsAsFactors = FALSE)

docs <- Corpus(VectorSource(data$RawText))
mode(docs)
```

## **Inspect a document**

```{r}
docs[[20]]
```

## **Preprocessing text**

```{r}
docs <- docs |>
         tm_map(removePunctuation) |>
         tm_map(content_transformer(tolower)) |> #to lower case
         tm_map(removeNumbers) |>
         tm_map(removeWords, stopwords("en")) |> #stopwords, such as a, an.
         tm_map(stripWhitespace) |>
         tm_map(stemDocument) #e.g. computer -> comput
```

```{r}
content(docs[[20]]) #note: stemming reduces a word to its ‘root’ with the aassumption that the ‘root’ represents the semantics of a word, e.g. computer, computing, computation, computers are about the concept of compute, which may be represented by ‘comput’. but stemming is never perfect.
```

Convert text to a matrix using `TF*IDF scores`

```{r}
DTData <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
DTData
```

```{r}
inspect(DTData[1:2, 1:5])
```

```{r}
TDData <- TermDocumentMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
inspect(TDData[1:2, 1:5])
```

## **Explore the dataset**

```{r}
findFreqTerms(TDData, lowfreq = 75, highfreq = 1000)
```

Find correlations among terms:

```{r}
findAssocs(TDData, terms = "bill", corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("bill"), corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("schedul"), corlimit = 0.3)
```

## **Create a word cloud**

```{r}
if (!require("wordcloud"))
install.packages("wordcloud")
if (!require("RColorBrewer"))
install.packages("RColorBrewer")
library(wordcloud)
library(RColorBrewer)
```

```{r}
data <- as.matrix(TDData)
freq <- sort(rowSums(data), decreasing = TRUE)
base <-data.frame(word = names(freq), freq = freq)
```

`png()` opens a new device \'png\' to output the graph to a local file

```{r}
# Save the word cloud as a PNG file
png(file = "wordCloud.png", width = 1000, height = 700, bg = "grey30")
wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
          random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
dev.off()  # Close the PNG device
```

```{r}
wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
```

```{r}
if (!require("onehot"))
install.packages("onehot")
library(onehot)
d <- data.frame(language=c("javascript", "python", "java"), hours=c(10, 3, 5) )
d$language <- as.factor(d$language) #convert the column to be encoded to Factor
encoded <- onehot(d)
new_d <- predict(encoded, d)
new_d
```

```{r}

```

```{r}
if (!require("qdapTools"))
install.packages("qdapTools")
library(qdapTools)
d <- data.frame(language=c("javascript, python", "java"), hours = c(3, 5) )
d
```

```{r}
dlist <- as.list(d)
new_d <- data.frame(cbind(dlist, mtabulate(strsplit(as.character(dlist$language), ", ")))) 

new_d
```

```{r}
dlist <- as.list(d)
new_d <- data.frame(cbind(dlist, mtabulate(strsplit(as.character(dlist$language), ", ")))) 

new_d
```

# **\[ADVANCED\]**

1.  What attributes are there in your data set?

```{r}
# Load the 'ggplot2' package and access the 'diamonds' dataset
library(ggplot2)
data(diamonds)

# Use the names() function to view the attributes
names(diamonds)

# Alternatively, you can use the colnames() function
colnames(diamonds)

summary(diamonds)
```

2.  Do you have highly correlated attributes? How did you find out about the correlations or lack of correlations

    -\> We will plot correlation plot for numeric attributes of the data set here it will be carat, depth, table & price for 'diamonds' data set.

```{r}
# Load the 'ggplot2' package and access the 'diamonds' dataset
library(ggplot2)
data(diamonds)

# Select only numeric columns
diamonds_numeric <- diamonds %>%
  select(carat, depth, table, price)

# Calculate the correlation matrix
correlation_matrix <- diamonds_numeric %>%
  drop_na() %>%
  cor()

# Load the 'corrplot' package
if (!require("corrplot"))
install.packages("corrplot") # Install the package if not already installed
library(corrplot)

# Create a basic correlation plot
corrplot(correlation_matrix, method = "color")

```

3.  Do you have numerical attributes that you might want to bin? Try at least two methods and compare the differences.

```{r}
# Load the 'ggplot2' package and access the 'diamonds' dataset
library(ggplot2)
data(diamonds)

# Binning using equal frequency
diamonds_equal_freq <- diamonds %>%
  mutate(carat_bin_equal_freq = cut(carat, quantile(carat, probs = seq(0, 1, by = 0.2)), labels = FALSE))

# Display the first few rows of the binned dataset
head(diamonds_equal_freq)

```

4.  If you have categorical attributes, use the concept hierarchy generation heuristics (based on attribute value counts) suggested in the textbook to produce some concept hierarchies. How well does this approach work for your attributes?

```{r}
# Load the 'ggplot2' package and access the 'diamonds' dataset
library(ggplot2)
data(diamonds)

# Generate a concept hierarchy for the 'cut' attribute
cut_hierarchy <- diamonds %>%
  group_by(cut) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

# Print the concept hierarchy
print(cut_hierarchy)

```
